---
layout: post
title: Hamiltonian Monte Carlo
author: prakrut
date: 2023-05-29 20:55:00 +1100
categories: [statistics]
tags: [stats]
math : true
---
HMC is a popular sampler for estimating the posterior densities in Bayesian analysis. I have resisted learning about it since long, disparaging it as yet another random walk algorithm. The ignorance finally started hurting me so here I am trying to understand it.

I am writing about it assuming undergrad level physics knowledge so I will skip all the trivialities of Legendre transforms.

The equilibrium distribution of a thermodynamical system is given by the Boltzmann distribution:

$$
P(q,p) = \frac{1}{Z} e^{ -\frac{H(p,q)}{k_{B} T} } \ ,
$$

where, $H(p,q)$ is the Hamiltonian.

# The HMC Algorithm
The HMC algorithm works as follows:
Define  the Hamiltonian as,

$$
H(p,q) = - \mathrm{ln} \, ( \pi \mathcal{L} ) + \frac{1}{2} \frac{p^2}{m} .
$$

The negative of log of the posterior is identified with the potential energy. We want the potential energy to be the lowest when the log-likelihood is the highest so that the sampler spends more time on average and gathers more samples in the high likelihood region. 

1. Choose a value of momentum $p_0$ from a zero-mean Normal distribution $\mathcal{N} (0, \sigma)$. The standard deviation is a tunable parameter of the algorithm.
2. Start from some point in the parameter space $q_0$. Evlolve the system with the Hamiltonian $H(p_0,q_0)$ and Hamiltons equations:

    $$
    \frac{ \partial H}{\partial q}  = -p \ ; \  \frac{ \partial H}{\partial p}  = q \ .
    $$

    Use finite difference integrators such a leap frog to integrate. The step size of the integrator and how long the system is allowed to evolve is determined by some hyperparameters of the algorithm.
3. Now reverse the momentum. At this point we have $p_1 = - p_0$. The current state of the system is $H(p_1,q_1)$. 
4. Accept the new state $H(p_1, q_1)$ with the Metropolis-Hastings acceptance probability A:

    $$
    A(p_1,q_1) = min \left[ 1, \  \frac{e^{-H(p_1, q_1)}}{ e^{-H(p_0, q_0)} } \right] \ .
    $$

    In other words, consider a Boltzmann distribution over the phase space and if the probability of the new point is greater than the probability of previous point, definitely accept it. Otherwise, accept it based on the ratio of probabilities of the two microstates. We have replaced the classical step of MH where we choose an arbitrary distribution with the Boltzmann distribution.

Repeat 1-4 multiple time to make chains.
Step 3 does not have any effect on the acceptance probability as the momentum appears quadratically in the Hamiltonian.


# Comments

HMC can be thought of as giving a particle some momentum at a point in the phase space and letting it evolve. Then after the system evolves for a while, we decide whether to accept or reject the point according to MH aceeptance probability where the proposal density is the Boltzmann distribution. The Boltzmann distribution ensures this method obeys the detailed balance condition and the equilibrium distribution is stable. 

The drawbacks of the algorithm are:
1. The tunable parameters - $\sigma$ of the momentum distribution, how long the sampler is allowed to evolve (L) and the step size of this evolution ($\epsilon$) are chosen without much guidance [^note1].
2. It is wasteful. To let the system evolve for some time, the likelihood needs to be evaluated quite a few times and all these intemediate samples are wasted.
3. The derivatiive of the likelihood needs to be computed which for some likelihoods might be too difficult to implement. 

Although, it is an efficient algorithm when compared to Metropolis-Hastings, I am not sure if it is worth the effort of writing a fully differentiable likelihood. Moreoever, it also suffers the curse of tunable parameters and there is no definite termination condition besides the convergence statistics. I remain skeptical of its efficiency when compared to something like Nested sampling.


# References and notes
[^note1]: There is at least one work which addresses this issue: [Selecting the Metric in Hamiltonian Monte Carlo by Ben Bales, Arya Pourzanjani, Aki Vehtari, Linda Petzold](https://arxiv.org/abs/1905.11916) and the related [blog post](https://statmodeling.stat.columbia.edu/2019/05/31/new-from-bales-pourzanjani-vehtari-petzold-selecting-the-metric-in-hamiltonian-monte-carlo/). Though, I still think it is quite painful to implement it properly.
